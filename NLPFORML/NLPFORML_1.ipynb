{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMfEcN5719PTY7y+D4l3BLM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Venu-16/My-Ai-Engineer-Journey/blob/main/NLPFORML_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**NLP Step-1**"
      ],
      "metadata": {
        "id": "1KWrc57LpEsJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WpdLCH4GnO7r",
        "outputId": "c9f5524a-0ea7-45d0-f924-3f04e1ecec9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.3)\n"
          ]
        }
      ],
      "source": [
        "pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = \"\"\"A small paragraph usually revolves around one main idea and is structured to be clear and coherent. It begins with a topic sentence that introduces the main point. followed by supporting sentences that provide details, examples, or explanations, and ends with a concluding sentence that summarizes the idea or reinforces the main point.\"\"\""
      ],
      "metadata": {
        "id": "f2RF9Q7bpVGA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize"
      ],
      "metadata": {
        "id": "SMKp7AlIqIIR"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')\n",
        "\n",
        "# Now run the tokenization again\n",
        "doc = sent_tokenize(corpus)\n",
        "print(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AGgJIjszqge6",
        "outputId": "701f0fc3-b812-45a9-fcf4-601e06d9e969"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['A small paragraph usually revolves around one main idea and is structured to be clear and coherent.', 'It begins with a topic sentence that introduces the main point.', 'followed by supporting sentences that provide details, examples, or explanations, and ends with a concluding sentence that summarizes the idea or reinforces the main point.']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bM8XRivmqeFM",
        "outputId": "3ec65857-9428-4377-ebfa-a506d41660ce"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "c-qTcz94qyLc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(corpus)"
      ],
      "metadata": {
        "id": "pl9UadQgrBjV"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XZrYumDrJA5",
        "outputId": "5e11e799-c43e-4da5-90b2-c77488566a31"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'small',\n",
              " 'paragraph',\n",
              " 'usually',\n",
              " 'revolves',\n",
              " 'around',\n",
              " 'one',\n",
              " 'main',\n",
              " 'idea',\n",
              " 'and',\n",
              " 'is',\n",
              " 'structured',\n",
              " 'to',\n",
              " 'be',\n",
              " 'clear',\n",
              " 'and',\n",
              " 'coherent',\n",
              " '.',\n",
              " 'It',\n",
              " 'begins',\n",
              " 'with',\n",
              " 'a',\n",
              " 'topic',\n",
              " 'sentence',\n",
              " 'that',\n",
              " 'introduces',\n",
              " 'the',\n",
              " 'main',\n",
              " 'point',\n",
              " '.',\n",
              " 'followed',\n",
              " 'by',\n",
              " 'supporting',\n",
              " 'sentences',\n",
              " 'that',\n",
              " 'provide',\n",
              " 'details',\n",
              " ',',\n",
              " 'examples',\n",
              " ',',\n",
              " 'or',\n",
              " 'explanations',\n",
              " ',',\n",
              " 'and',\n",
              " 'ends',\n",
              " 'with',\n",
              " 'a',\n",
              " 'concluding',\n",
              " 'sentence',\n",
              " 'that',\n",
              " 'summarizes',\n",
              " 'the',\n",
              " 'idea',\n",
              " 'or',\n",
              " 'reinforces',\n",
              " 'the',\n",
              " 'main',\n",
              " 'point',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8gnY35QXrLXY",
        "outputId": "2900e1f8-1e36-4a4d-b23c-4e1192ed1082"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_from_sentences = [word_tokenize(sent) for sent in doc]"
      ],
      "metadata": {
        "id": "zob7cxu2rNac"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words_from_sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBuVLkuUraN-",
        "outputId": "8f5c2972-78de-4d14-88c8-663fe246da59"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['A',\n",
              "  'small',\n",
              "  'paragraph',\n",
              "  'usually',\n",
              "  'revolves',\n",
              "  'around',\n",
              "  'one',\n",
              "  'main',\n",
              "  'idea',\n",
              "  'and',\n",
              "  'is',\n",
              "  'structured',\n",
              "  'to',\n",
              "  'be',\n",
              "  'clear',\n",
              "  'and',\n",
              "  'coherent',\n",
              "  '.'],\n",
              " ['It',\n",
              "  'begins',\n",
              "  'with',\n",
              "  'a',\n",
              "  'topic',\n",
              "  'sentence',\n",
              "  'that',\n",
              "  'introduces',\n",
              "  'the',\n",
              "  'main',\n",
              "  'point',\n",
              "  '.'],\n",
              " ['followed',\n",
              "  'by',\n",
              "  'supporting',\n",
              "  'sentences',\n",
              "  'that',\n",
              "  'provide',\n",
              "  'details',\n",
              "  ',',\n",
              "  'examples',\n",
              "  ',',\n",
              "  'or',\n",
              "  'explanations',\n",
              "  ',',\n",
              "  'and',\n",
              "  'ends',\n",
              "  'with',\n",
              "  'a',\n",
              "  'concluding',\n",
              "  'sentence',\n",
              "  'that',\n",
              "  'summarizes',\n",
              "  'the',\n",
              "  'idea',\n",
              "  'or',\n",
              "  'reinforces',\n",
              "  'the',\n",
              "  'main',\n",
              "  'point',\n",
              "  '.']]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "type(words_from_sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jvkNxEBksqWU",
        "outputId": "ae7cfc81-d069-442e-b000-a3183aede06f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize"
      ],
      "metadata": {
        "id": "Rj4SAvx7rz6r"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordsByWordPunct = wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "id": "dUv3kquSsJvC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordsByWordPunct"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdOgwtXfsJrs",
        "outputId": "16cbc83f-9296-4082-d82b-a85aaae8aba7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'small',\n",
              " 'paragraph',\n",
              " 'usually',\n",
              " 'revolves',\n",
              " 'around',\n",
              " 'one',\n",
              " 'main',\n",
              " 'idea',\n",
              " 'and',\n",
              " 'is',\n",
              " 'structured',\n",
              " 'to',\n",
              " 'be',\n",
              " 'clear',\n",
              " 'and',\n",
              " 'coherent',\n",
              " '.',\n",
              " 'It',\n",
              " 'begins',\n",
              " 'with',\n",
              " 'a',\n",
              " 'topic',\n",
              " 'sentence',\n",
              " 'that',\n",
              " 'introduces',\n",
              " 'the',\n",
              " 'main',\n",
              " 'point',\n",
              " '.',\n",
              " 'followed',\n",
              " 'by',\n",
              " 'supporting',\n",
              " 'sentences',\n",
              " 'that',\n",
              " 'provide',\n",
              " 'details',\n",
              " ',',\n",
              " 'examples',\n",
              " ',',\n",
              " 'or',\n",
              " 'explanations',\n",
              " ',',\n",
              " 'and',\n",
              " 'ends',\n",
              " 'with',\n",
              " 'a',\n",
              " 'concluding',\n",
              " 'sentence',\n",
              " 'that',\n",
              " 'summarizes',\n",
              " 'the',\n",
              " 'idea',\n",
              " 'or',\n",
              " 'reinforces',\n",
              " 'the',\n",
              " 'main',\n",
              " 'point',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer"
      ],
      "metadata": {
        "id": "UzgZtiUIsJpX"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "wordsbyTreebank = tokenizer.tokenize(corpus)\n",
        "wordsbyTreebank"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3TTeJ4MsJmY",
        "outputId": "864d9931-fe5f-4547-eef0-4cf623f67182"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['A',\n",
              " 'small',\n",
              " 'paragraph',\n",
              " 'usually',\n",
              " 'revolves',\n",
              " 'around',\n",
              " 'one',\n",
              " 'main',\n",
              " 'idea',\n",
              " 'and',\n",
              " 'is',\n",
              " 'structured',\n",
              " 'to',\n",
              " 'be',\n",
              " 'clear',\n",
              " 'and',\n",
              " 'coherent.',\n",
              " 'It',\n",
              " 'begins',\n",
              " 'with',\n",
              " 'a',\n",
              " 'topic',\n",
              " 'sentence',\n",
              " 'that',\n",
              " 'introduces',\n",
              " 'the',\n",
              " 'main',\n",
              " 'point.',\n",
              " 'followed',\n",
              " 'by',\n",
              " 'supporting',\n",
              " 'sentences',\n",
              " 'that',\n",
              " 'provide',\n",
              " 'details',\n",
              " ',',\n",
              " 'examples',\n",
              " ',',\n",
              " 'or',\n",
              " 'explanations',\n",
              " ',',\n",
              " 'and',\n",
              " 'ends',\n",
              " 'with',\n",
              " 'a',\n",
              " 'concluding',\n",
              " 'sentence',\n",
              " 'that',\n",
              " 'summarizes',\n",
              " 'the',\n",
              " 'idea',\n",
              " 'or',\n",
              " 'reinforces',\n",
              " 'the',\n",
              " 'main',\n",
              " 'point',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fd40c767"
      },
      "source": [
        "### Comparison of Tokenization Methods\n",
        "\n",
        "| Feature | `word_tokenize` | `wordpunct_tokenize` | `TreebankWordTokenizer` |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **Core Logic** | Punkt Model (heuristic) | Regex-based (whitespace/char) | Penn Treebank Rules |\n",
        "| **Punctuation** | Usually separated | Always separated | Separates most, but rules vary |\n",
        "| **Contractions** | Splits (e.g., \"don't\" -> \"do\", \"n't\") | Splits by punct (e.g., \"don\", \"'\", \"t\") | Splits (e.g., \"don't\" -> \"do\", \"n't\") |\n",
        "| **Speed** | Moderate | Very Fast | Fast |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "10241834",
        "outputId": "c1aefc5e-672d-4635-83c7-274487db51e4"
      },
      "source": [
        "test_input = \"I can't wait for the NLP session. It's great!\"\n",
        "\n",
        "print(f\"Original: {test_input}\\n\")\n",
        "print(f\"word_tokenize:\\n{word_tokenize(test_input)}\\n\")\n",
        "print(f\"wordpunct_tokenize:\\n{wordpunct_tokenize(test_input)}\\n\")\n",
        "\n",
        "treebank = TreebankWordTokenizer()\n",
        "print(f\"TreebankWordTokenizer:\\n{treebank.tokenize(test_input)}\")"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original: I can't wait for the NLP session. It's great!\n",
            "\n",
            "word_tokenize:\n",
            "['I', 'ca', \"n't\", 'wait', 'for', 'the', 'NLP', 'session', '.', 'It', \"'s\", 'great', '!']\n",
            "\n",
            "wordpunct_tokenize:\n",
            "['I', 'can', \"'\", 't', 'wait', 'for', 'the', 'NLP', 'session', '.', 'It', \"'\", 's', 'great', '!']\n",
            "\n",
            "TreebankWordTokenizer:\n",
            "['I', 'ca', \"n't\", 'wait', 'for', 'the', 'NLP', 'session.', 'It', \"'s\", 'great', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Tokenization Complete**"
      ],
      "metadata": {
        "id": "_zKX3cGIrmvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**STEMMING**"
      ],
      "metadata": {
        "id": "TXj55DrVtewS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer"
      ],
      "metadata": {
        "id": "y2bZFT22rcEf"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "words = ['A',\n",
        " 'small',\n",
        " 'paragraph',\n",
        " 'usually',\n",
        " 'revolves',\n",
        " 'around',\n",
        " 'one',\n",
        " 'main',\n",
        " 'idea',\n",
        " 'and',\n",
        " 'is',\n",
        " 'structured',\n",
        " 'to',\n",
        " 'be',\n",
        " 'clear',\n",
        " 'and',\n",
        " 'coherent.',\n",
        " 'It',\n",
        " 'begins',\n",
        " 'with',\n",
        " 'a',\n",
        " 'topic',\n",
        " 'sentence',\n",
        " 'that',\n",
        " 'introduces',\n",
        " 'the',\n",
        " 'main',\n",
        " 'point.',\n",
        " 'followed',\n",
        " 'by',\n",
        " 'supporting',\n",
        " 'sentences',\n",
        " 'that',\n",
        " 'provide',\n",
        " 'details',\n",
        " 'examples',\n",
        " 'or',\n",
        " 'explanations',\n",
        " 'and',\n",
        " 'ends',\n",
        " 'with',\n",
        " 'a',\n",
        " 'concluding',\n",
        " 'sentence',\n",
        " 'that',\n",
        " 'summarizes',\n",
        " 'the',\n",
        " 'idea',\n",
        " 'or',\n",
        " 'reinforces',\n",
        " 'the',\n",
        " 'main',\n",
        " 'point']"
      ],
      "metadata": {
        "id": "3zTB39skttTf"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word+\"  ------>>>>>  \"+PorterStemmer().stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMqYVtiTuAhL",
        "outputId": "0044474f-61ec-4e5c-df0f-6befa2a1a7df"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A  ------>>>>>  a\n",
            "small  ------>>>>>  small\n",
            "paragraph  ------>>>>>  paragraph\n",
            "usually  ------>>>>>  usual\n",
            "revolves  ------>>>>>  revolv\n",
            "around  ------>>>>>  around\n",
            "one  ------>>>>>  one\n",
            "main  ------>>>>>  main\n",
            "idea  ------>>>>>  idea\n",
            "and  ------>>>>>  and\n",
            "is  ------>>>>>  is\n",
            "structured  ------>>>>>  structur\n",
            "to  ------>>>>>  to\n",
            "be  ------>>>>>  be\n",
            "clear  ------>>>>>  clear\n",
            "and  ------>>>>>  and\n",
            "coherent.  ------>>>>>  coherent.\n",
            "It  ------>>>>>  it\n",
            "begins  ------>>>>>  begin\n",
            "with  ------>>>>>  with\n",
            "a  ------>>>>>  a\n",
            "topic  ------>>>>>  topic\n",
            "sentence  ------>>>>>  sentenc\n",
            "that  ------>>>>>  that\n",
            "introduces  ------>>>>>  introduc\n",
            "the  ------>>>>>  the\n",
            "main  ------>>>>>  main\n",
            "point.  ------>>>>>  point.\n",
            "followed  ------>>>>>  follow\n",
            "by  ------>>>>>  by\n",
            "supporting  ------>>>>>  support\n",
            "sentences  ------>>>>>  sentenc\n",
            "that  ------>>>>>  that\n",
            "provide  ------>>>>>  provid\n",
            "details  ------>>>>>  detail\n",
            "examples  ------>>>>>  exampl\n",
            "or  ------>>>>>  or\n",
            "explanations  ------>>>>>  explan\n",
            "and  ------>>>>>  and\n",
            "ends  ------>>>>>  end\n",
            "with  ------>>>>>  with\n",
            "a  ------>>>>>  a\n",
            "concluding  ------>>>>>  conclud\n",
            "sentence  ------>>>>>  sentenc\n",
            "that  ------>>>>>  that\n",
            "summarizes  ------>>>>>  summar\n",
            "the  ------>>>>>  the\n",
            "idea  ------>>>>>  idea\n",
            "or  ------>>>>>  or\n",
            "reinforces  ------>>>>>  reinforc\n",
            "the  ------>>>>>  the\n",
            "main  ------>>>>>  main\n",
            "point  ------>>>>>  point\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WSWR43_kuRC0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d498b44f",
        "outputId": "b0bb64fd-db3e-4108-ce1b-0a613a976f35"
      },
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "\n",
        "# Initialize with a regex pattern and a minimum length for the result stem\n",
        "# The pattern below identifies 'ing', 'ly', and 'ed' at the end of words\n",
        "regexp_stemmer = RegexpStemmer('ing$|ly$|ed$', min=4)\n",
        "\n",
        "test_words = ['usually', 'revolves', 'structured', 'concluding', 'reinforces']\n",
        "\n",
        "print(f\"{'Word':<15} | {'Regexp Stem':<15}\")\n",
        "print('-' * 35)\n",
        "for w in test_words:\n",
        "    print(f\"{w:<15} | {regexp_stemmer.stem(w):<15}\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word            | Regexp Stem    \n",
            "-----------------------------------\n",
            "usually         | usual          \n",
            "revolves        | revolves       \n",
            "structured      | structur       \n",
            "concluding      | conclud        \n",
            "reinforces      | reinforces     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Snow ball stemmer"
      ],
      "metadata": {
        "id": "NrgEVxsuvPTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer"
      ],
      "metadata": {
        "id": "5MQ-gou-vMtD"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(SnowballStemmer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AhY_9xVEvblT",
        "outputId": "300c2def-62a9-4471-9d45-56fcf126ea5e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class SnowballStemmer in module nltk.stem.snowball:\n",
            "\n",
            "class SnowballStemmer(nltk.stem.api.StemmerI)\n",
            " |  SnowballStemmer(language, ignore_stopwords=False)\n",
            " |\n",
            " |  Snowball Stemmer\n",
            " |\n",
            " |  The following languages are supported:\n",
            " |  Arabic, Danish, Dutch, English, Finnish, French, German,\n",
            " |  Hungarian, Italian, Norwegian, Portuguese, Romanian, Russian,\n",
            " |  Spanish and Swedish.\n",
            " |\n",
            " |  The algorithm for English is documented here:\n",
            " |\n",
            " |      Porter, M. \"An algorithm for suffix stripping.\"\n",
            " |      Program 14.3 (1980): 130-137.\n",
            " |\n",
            " |  The algorithms have been developed by Martin Porter.\n",
            " |  These stemmers are called Snowball, because Porter created\n",
            " |  a programming language with this name for creating\n",
            " |  new stemming algorithms. There is more information available\n",
            " |  at http://snowball.tartarus.org/\n",
            " |\n",
            " |  The stemmer is invoked as shown below:\n",
            " |\n",
            " |  >>> from nltk.stem import SnowballStemmer # See which languages are supported\n",
            " |  >>> print(\" \".join(SnowballStemmer.languages)) # doctest: +NORMALIZE_WHITESPACE\n",
            " |  arabic danish dutch english finnish french german hungarian\n",
            " |  italian norwegian porter portuguese romanian russian\n",
            " |  spanish swedish\n",
            " |  >>> stemmer = SnowballStemmer(\"german\") # Choose a language\n",
            " |  >>> stemmer.stem(\"Autobahnen\") # Stem a word\n",
            " |  'autobahn'\n",
            " |\n",
            " |  Invoking the stemmers that way is useful if you do not know the\n",
            " |  language to be stemmed at runtime. Alternatively, if you already know\n",
            " |  the language, then you can invoke the language specific stemmer directly:\n",
            " |\n",
            " |  >>> from nltk.stem.snowball import GermanStemmer\n",
            " |  >>> stemmer = GermanStemmer()\n",
            " |  >>> stemmer.stem(\"Autobahnen\")\n",
            " |  'autobahn'\n",
            " |\n",
            " |  :param language: The language whose subclass is instantiated.\n",
            " |  :type language: str or unicode\n",
            " |  :param ignore_stopwords: If set to True, stopwords are\n",
            " |                           not stemmed and returned unchanged.\n",
            " |                           Set to False by default.\n",
            " |  :type ignore_stopwords: bool\n",
            " |  :raise ValueError: If there is no stemmer for the specified\n",
            " |                         language, a ValueError is raised.\n",
            " |\n",
            " |  Method resolution order:\n",
            " |      SnowballStemmer\n",
            " |      nltk.stem.api.StemmerI\n",
            " |      builtins.object\n",
            " |\n",
            " |  Methods defined here:\n",
            " |\n",
            " |  __init__(self, language, ignore_stopwords=False)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |\n",
            " |  stem(self, token)\n",
            " |      Strip affixes from the token and return the stem.\n",
            " |\n",
            " |      :param token: The token that should be stemmed.\n",
            " |      :type token: str\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |\n",
            " |  __abstractmethods__ = frozenset()\n",
            " |\n",
            " |  languages = ('arabic', 'danish', 'dutch', 'english', 'finnish', 'frenc...\n",
            " |\n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from nltk.stem.api.StemmerI:\n",
            " |\n",
            " |  __dict__\n",
            " |      dictionary for instance variables\n",
            " |\n",
            " |  __weakref__\n",
            " |      list of weak references to the object\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowstemmer = SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "lbw5LunBvd4X"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_words = ['usually', 'revolves', 'structured', 'concluding', 'reinforces']"
      ],
      "metadata": {
        "id": "su7dflLlvzCc"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in test_words:\n",
        "  print(word+\"  ------>>>>>  \"+snowstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqVpQ0Q6v4tp",
        "outputId": "260cb089-e615-4d76-ad2b-d5008666d876"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usually  ------>>>>>  usual\n",
            "revolves  ------>>>>>  revolv\n",
            "structured  ------>>>>>  structur\n",
            "concluding  ------>>>>>  conclud\n",
            "reinforces  ------>>>>>  reinforc\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VfBY03GJv8GW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33c8653e"
      },
      "source": [
        "### Comparison of Stemming Methods\n",
        "\n",
        "| Stemmer | Logic | Multi-language? | Best Use Case |\n",
        "| :--- | :--- | :--- | :--- |\n",
        "| **PorterStemmer** | 5-step heuristic rules | No (English only) | General purpose English NLP |\n",
        "| **RegexpStemmer** | User-defined Regex | Yes (Depends on Regex) | Removing specific suffixes/prefixes |\n",
        "| **SnowballStemmer** | Improved Porter logic | Yes (15+ languages) | Production use, non-English text |"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77aa570f",
        "outputId": "62978492-9ead-43b7-a430-948829d6a73a"
      },
      "source": [
        "from nltk.stem import PorterStemmer, SnowballStemmer, RegexpStemmer\n",
        "\n",
        "porter = PorterStemmer()\n",
        "snowball = SnowballStemmer('english')\n",
        "regexp = RegexpStemmer('ing$|ly$|ed$', min=4)\n",
        "\n",
        "comparison_words = ['fairly', 'sporting', 'revolves', 'caress', 'ponies']\n",
        "\n",
        "print(f\"{'Word':<12} | {'Porter':<12} | {'Snowball':<12} | {'Regexp':<12}\")\n",
        "print('-' * 55)\n",
        "for w in comparison_words:\n",
        "    p_stem = porter.stem(w)\n",
        "    s_stem = snowball.stem(w)\n",
        "    r_stem = regexp.stem(w)\n",
        "    print(f\"{w:<12} | {p_stem:<12} | {s_stem:<12} | {r_stem:<12}\")"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word         | Porter       | Snowball     | Regexp      \n",
            "-------------------------------------------------------\n",
            "fairly       | fairli       | fair         | fair        \n",
            "sporting     | sport        | sport        | sport       \n",
            "revolves     | revolv       | revolv       | revolves    \n",
            "caress       | caress       | caress       | caress      \n",
            "ponies       | poni         | poni         | ponies      \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LEMMATIZATION**"
      ],
      "metadata": {
        "id": "1NCm5l-gwTQX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Ees9_UnEwXOi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6e09a3bb"
      },
      "source": [
        "### What is Lemmatization?\n",
        "\n",
        "Lemmatization is the process of grouping together the inflected forms of a word so they can be analyzed as a single item, identified by the word's **lemma**, or dictionary form.\n",
        "\n",
        "#### Key Differences from Stemming:\n",
        "* **Accuracy**: Stemming often creates non-existent words (e.g., 'studies' -> 'studi'), while Lemmatization always returns a valid word (e.g., 'studies' -> 'study').\n",
        "* **Context Awareness**: Lemmatization can use Part-of-Speech (POS) tags to distinguish between words that look the same but have different meanings (e.g., 'meeting' as a noun vs. 'meeting' as a verb).\n",
        "* **Complexity**: Lemmatization is computationally more expensive because it involves looking up a dictionary (lexicon).\n",
        "\n",
        "#### Important Points:\n",
        "1. **WordNet**: NLTK typically uses the `WordNetLemmatizer` which relies on the WordNet lexical database.\n",
        "2. **POS Tagging**: By default, most lemmatizers assume words are nouns. For better results, you should provide the POS tag (e.g., 'v' for verb, 'a' for adjective).\n",
        "3. **Requirements**: You usually need to download the `wordnet` and `omw-1.4` resources in NLTK to use it."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1e76463e",
        "outputId": "0ca51fe4-94de-4fd8-c04a-3e38a01e0f9a"
      },
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download necessary resources for lemmatization\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TstWd151wlwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d616908",
        "outputId": "886c4350-9eb1-4fe7-d122-652ac1fcc07f"
      },
      "source": [
        "# Examples of lemmatization\n",
        "words_to_lemmatize = [\"running\", \"ate\", \"better\", \"rocks\", \"corpora\"]\n",
        "\n",
        "print(f\"{'Word':<12} | {'Noun Lemma (default)':<20} | {'Verb Lemma':<12}\")\n",
        "print('-' * 55)\n",
        "for w in words_to_lemmatize:\n",
        "    n_lemma = lemmatizer.lemmatize(w, pos='n')\n",
        "    v_lemma = lemmatizer.lemmatize(w, pos='v')\n",
        "    print(f\"{w:<12} | {n_lemma:<20} | {v_lemma:<12}\")"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word         | Noun Lemma (default) | Verb Lemma  \n",
            "-------------------------------------------------------\n",
            "running      | running              | run         \n",
            "ate          | ate                  | eat         \n",
            "better       | better               | better      \n",
            "rocks        | rock                 | rock        \n",
            "corpora      | corpus               | corpora     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2f550ea"
      },
      "source": [
        "### Advantages and Disadvantages: Lemmatization vs. Stemming\n",
        "\n",
        "#### **Lemmatization**\n",
        "*   **Advantages:**\n",
        "    *   **Accuracy**: Returns a valid dictionary word (the 'lemma').\n",
        "    *   **Contextual**: Can distinguish between meanings based on POS tags (e.g., 'saw' as a noun vs. 'see' as a verb).\n",
        "    *   **Better for SEO/Search**: Since it uses real words, it's better for applications where user-facing text or dictionary matches are important.\n",
        "*   **Disadvantages:**\n",
        "    *   **Speed**: Much slower than stemming because it requires a dictionary lookup.\n",
        "    *   **Complexity**: Requires more computational resources and often requires prior POS tagging for best results.\n",
        "\n",
        "#### **Stemming**\n",
        "*   **Advantages:**\n",
        "    *   **Speed**: Extremely fast; it uses simple rule-based suffix stripping.\n",
        "    *   **Simplicity**: Easy to implement and doesn't require complex linguistic databases.\n",
        "    *   **Memory**: Low memory footprint as there is no large lexicon to load.\n",
        "*   **Disadvantages:**\n",
        "    *   **Over-stemming**: Cutting off too much (e.g., 'universal' -> 'univers').\n",
        "    *   **Under-stemming**: Not cutting enough (e.g., 'data' and 'datum' remaining separate).\n",
        "    *   **Non-words**: Often results in stems that are not actual words (e.g., 'studies' -> 'studi')."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Stop words**"
      ],
      "metadata": {
        "id": "r_JJJhu4xKM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "Pke5CiE9xJvn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gXDKnfiZxVLn",
        "outputId": "746e7fa7-24d1-4288-df49-0d3608092ca1"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = set(stopwords.words('english'))"
      ],
      "metadata": {
        "id": "ML7EGABpxZ4O"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_corpus = [word for word in words if word.lower() not in stopwords]"
      ],
      "metadata": {
        "id": "Kc0z3qUux4b7"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4kf8t4Lx4YS",
        "outputId": "89811bb5-2e79-4cbc-ebf4-a57f768daee8"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['small',\n",
              " 'paragraph',\n",
              " 'usually',\n",
              " 'revolves',\n",
              " 'around',\n",
              " 'one',\n",
              " 'main',\n",
              " 'idea',\n",
              " 'structured',\n",
              " 'clear',\n",
              " 'coherent.',\n",
              " 'begins',\n",
              " 'topic',\n",
              " 'sentence',\n",
              " 'introduces',\n",
              " 'main',\n",
              " 'point.',\n",
              " 'followed',\n",
              " 'supporting',\n",
              " 'sentences',\n",
              " 'provide',\n",
              " 'details',\n",
              " 'examples',\n",
              " 'explanations',\n",
              " 'ends',\n",
              " 'concluding',\n",
              " 'sentence',\n",
              " 'summarizes',\n",
              " 'idea',\n",
              " 'reinforces',\n",
              " 'main',\n",
              " 'point']"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "id": "qzsfoxjUx4VW",
        "outputId": "b932c55a-a88e-46d6-f896-69337b72805c"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A small paragraph usually revolves around one main idea and is structured to be clear and coherent. It begins with a topic sentence that introduces the main point. followed by supporting sentences that provide details, examples, or explanations, and ends with a concluding sentence that summarizes the idea or reinforces the main point.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    }
  ]
}